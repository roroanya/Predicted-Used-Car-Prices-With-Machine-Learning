# -*- coding: utf-8 -*-
"""Used Car Prediction GDGOC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJP8onIdD1KZ6e61b2QnhUdUjW-zmH4U

#Data Preparation
Here goes EDA, DATA Preprocessing and Feature engineering

##EDA

Memahami data dari awal dari segi statistik nya, karakteristik data nya, tren dan pola data nya.

Further references : https://docs.google.com/presentation/d/1P-yXwal4BvNjELxeOdsquXRAg3EXDjBE/edit?usp=sharing&ouid=111794804156151213377&rtpof=true&sd=true
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

!gdown --id 1kRjo7M6AbUm320kcnDHuXrPNEuETYsUi -O second_hand_cars_price.csv

df = pd.read_csv('second_hand_cars_price.csv')
df.head()

# Variabel target yang ingin diprediksi adalah 'Price'
target_variable = 'Price'
y = df[target_variable]

price_rows = df[df['Price'].notnull()]
print(price_rows.head())
print("Jumlah data dengan harga tersedia:", price_rows.shape[0])

df.head()

print("Bentuk dataset:", df.shape)

df.info()

df.describe()

#Cek duplicate values
df.duplicated().sum()

#Cek missing values
df.isnull().sum()

sns.histplot(df['Kilometers_Driven'], kde=True)
plt.title("Histogram Distribusi Kilometers Driven")
plt.xlabel("Kilometers Driven")
plt.ylabel("Frekuensi")
plt.show()

if (df['Kilometers_Driven'] > 0).all():
    df['Kilometers_Driven_log'] = np.log(df['Kilometers_Driven'])
else:
    df['Kilometers_Driven_log'] = np.log(df['Kilometers_Driven'] + 1)

df['Kilometers_Driven_log'].head()

df.head()

df.describe()

df.describe(include=['object', 'bool'])

df.shape

#Bivariate Analysis
correlation_matrix = df.select_dtypes(include=[np.number]).corr()
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Matriks Korelasi')
plt.show()

#Multivariate Analysis
pivot_result = pd.pivot_table(data=df, index='Year', columns='Fuel_Type', values='Price')

plt.figure(figsize=(12, 6))
sns.heatmap(pivot_result, annot=True, cmap='coolwarm')
plt.title("Rata-rata Harga berdasarkan Tahun dan Tipe Bahan Bakar")
plt.show()

#Boxplot untuk deteksi Outlier
numerical_columns = df.select_dtypes(include=[np.number]).columns

plt.figure(figsize=(15, 8))
df[numerical_columns].boxplot(rot=45)
plt.title("Boxplot untuk Deteksi Outlier")
plt.show()

#Boxplot
numerical_columns = df.select_dtypes(include=[np.number]).columns
for column in numerical_columns:
    plt.boxplot(df[column])
    plt.title(f"Boxplot of {column}")
    plt.show()

outlier_features = []
for col in numerical_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    if df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0] > 0:
        outlier_features.append(col)

print("Fitur yang memiliki outlier:", outlier_features)

categorical_columns = df.select_dtypes(include=['object']).columns

for col in categorical_columns:
    print(f"Fitur '{col}' memiliki {df[col].nunique()} nilai unik")
    print("nilai:", df[col].unique()[:5])
    print("-" * 40)

plt.figure(figsize=(8, 5))
sns.scatterplot(x=df["Kilometers_Driven"], y=df["Price"])
plt.title("Scatter Plot: Kilometers_Driven vs Price")
plt.xlabel("Kilometers Driven")
plt.ylabel("Price")
plt.show()

# Filter data yang lengkap
df = df[df['Fuel_Type'].notnull() & df['Transmission'].notnull() & df['Price'].notnull()]

# Hitung rata-rata harga berdasarkan kombinasi Fuel_Type dan Transmission
avg_price_combo = df.groupby(['Fuel_Type', 'Transmission'])['Price'].mean().reset_index()

# Lihat hasil data
print("Rata-rata harga berdasarkan Fuel Type dan Transmission:")
print(avg_price_combo)

# Buat barplot
plt.figure(figsize=(10,6))
sns.barplot(x='Fuel_Type', y='Price', hue='Transmission', data=avg_price_combo, palette='mako')
plt.title('Rata-rata Harga Mobil Berdasarkan Bahan Bakar dan Transmisi')
plt.xlabel('Jenis Bahan Bakar')
plt.ylabel('Harga Rata-rata')
plt.legend(title='Transmisi')
plt.tight_layout()
plt.show()

"""##Data Preprocessing"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

!gdown --id 1kRjo7M6AbUm320kcnDHuXrPNEuETYsUi -O second_hand_cars_price.csv

df = pd.read_csv('second_hand_cars_price.csv')

df.describe()

# Mengidentifikasi Mean-Median-Modus
print("Mean:", df['Price'].mean())
print("Median:", df['Price'].median())
print("Modus:", df['Price'].mode()[0])

##Handling Dirty Data
#Cek data duplikat
print("Jumlah duplikat:", df.duplicated().sum())

# Mengecek Missing Value
df.isnull().sum()

categorical_features = [feature for feature in df.columns if df[feature].dtype == 'object']
print('The feature is',len(categorical_features))
print('The features are',categorical_features)

#binary encoding (mengubah categorical ke numerical), karena menggunakan encoding binary membuat data lebih mudah diproses dan disimpan oleh perangkat digital
#menggunakan . map() to convert for fuel_type and transmission
#menggunakan .fillna(-1) untuk data yang missing

df = pd.read_csv('second_hand_cars_price.csv')

df["Brand"] = df["Brand"].astype("category").cat.codes
df["Model"] = df["Model"].astype("category").cat.codes
df["Fuel_Type"] = df["Fuel_Type"].map({'Petrol': 1, 'Diesel': 0}).fillna(-1).astype("int64")
df["Transmission"] = df["Transmission"].map({'Manual': 1, 'Automatic': 0}).fillna(-1).astype("int64")
df["Owner_Type"] = df["Owner_Type"].map({'First': 1, 'Second': 2, 'Third': 3}).fillna(-1).astype("int64")

df.info()

df.head(5)

#outlier detection using IQR, jadi yang ada outliernya Year, Kilometers_Driven, Engine, Power, Seats
Q1 = df.select_dtypes(include=[np.number]).quantile(0.25)
Q3 = df.select_dtypes(include=[np.number]).quantile(0.75)
IQR = Q3 - Q1

batas_bawah = Q1 - 1.5 * IQR
batas_atas = Q3 + 1.5 * IQR

outliers = df[(df.select_dtypes(include=[np.number]) < batas_bawah) | (df.select_dtypes(include=[np.number]) > batas_atas)]
print("Outlier berdasarkan IQR:")
print(outliers.sum())

df.dtypes

# #Normalization and Data Scaling menggunakan Standard Scaler

# # transform Standard Scaler
# #from sklearn.preprocessing import StandardScaler
# #standard_scaler = StandardScaler()
# #df_standard = pd.DataFrame(standard_scaler.fit_transform(df.select_dtypes(include=[np.number])))
# #standardScaler_data = standard_scaler.transform(df.select_dtypes(include=[np.number]))
# #df_standard = pd.DataFrame(standardScaler_data, columns=df.select_dtypes(include=[np.number]).columns)

# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler

# # 1. Pisahkan fitur numerik dan target
# X = df.drop(columns='Price')
# X_num = X.select_dtypes(include=np.number)
# y = df['Price']

# # 2. Inisialisasi Standard Scaler
# standard_scaler = StandardScaler()

# # 3. Fit dan transform data fitur
# X_standard = standard_scaler.fit_transform(X_num)

# # 4. Buat DataFrame baru dengan kolom asli
# df_standard = pd.DataFrame(X_standard, columns=X_num.columns)

# # 5. (Opsional) Jika ingin gabungkan kembali target 'price'
# df_standard['Price'] = y

# df_standard.head(5)

df.head(5)

# df = df_standard

df.head(5)

# Cek skewness semua kolom numerik
numeric_cols = df.select_dtypes(include='number').columns
print("Skewness awal:")
print(df[numeric_cols].skew())

import numpy as np
# Menggunakan log transformation (np.log1p) untuk mengurangi skewness pada fitur numerik
# sehingga distribusi lebih simetris dan model dapat bekerja lebih optimal
df['Engine'] = np.log1p(df['Engine'])
df['Price'] = np.log1p(df['Price'])
df['Power'] = np.log1p(df['Power'])
df['Seats'] = np.log1p(df['Seats'])

# Cek hasil setelah transform
print("Skewness setelah transformasi:")
print(df[['Engine', 'Power', 'Seats', 'Price']].skew())

import seaborn as sns
import matplotlib.pyplot as plt

# Visualisasi distribusi semua fitur numerik setelah transformasi
numerical_cols = df.select_dtypes(include=np.number).columns

for i in numerical_cols:
    # Plot histogram dan KDE
    sns.histplot(df[i], kde=True)
    plt.title(f'Distribusi data {i}')
    plt.xlabel(i)
    plt.ylabel('Frekuensi')
    plt.show()

# Cek skewness
    skewness = df[i].skew()
    print(f'Skewness: {skewness:.4f}')

# Statistik deskriptif
    print("Median:", df[i].median())
    print("Modus :", df[i].mode()[0])
    print("Mean  :", df[i].mean())

# Interpretasi skew
    if skewness > 0.5:
        print('→ Distribusi data cenderung right-skewed (positive skew).')
    elif skewness < -0.5:
        print('→ Distribusi data cenderung left-skewed (negative skew).')
    else:
        print('→ Distribusi data cenderung simetris (mendekati Gaussian).')

    print('-' * 50)

# Outlier Handling menggunakan mean/median

# Outlier detection using IQR and Z score
# IQR
numeric_cols = df.select_dtypes(include=np.number).columns

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
  # Batas bawah dan atas
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Indeks outlier
    outlier_index = (df[col] < lower_bound) | (df[col] > upper_bound)

    # Pengganti outlier (mean/median)
    replacement_value = df[col].median()

    # Imputasi
    df.loc[outlier_index, col] = replacement_value
print("DataFrame setelah Mean/Median Imputation pada Outlier (IQR):")
df.head(5)

#check outlier semua feature numerikal menggunakan boxplot
import matplotlib.pyplot as plt
import numpy as np

numerical_columns = df.select_dtypes(include=[np.number]).columns

for column in numerical_columns:
  plt.boxplot(df[column])
  plt.title(f"Boxplot of {column}")
  plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Plot distribusi data 'Seats'
sns.histplot(df['Seats'], kde=True)
plt.title('Distribusi Data Seats')
plt.xlabel('Seats')
plt.ylabel('Frekuensi')
plt.show()

# Cek skewness
skewness = df['Seats'].skew()
print(f'Skewness: {skewness}')

# Interpretasi skewness
if skewness > 0:
    print('Distribusi data Seats cenderung right-skewed (positive skew).')
elif skewness < 0:
    print('Distribusi data Seats cenderung left-skewed (negative skew).')
else:
    print('Distribusi data Seats cenderung simetris (mendekati Gaussian).')

"""Skewness diartikan sebagai kemiringan distribusi data. Sebuah distribusi yang tidak simetris akan memiliki rata-rata, median, dan modus yang tidak sama besarnya sehingga distribusi akan terkonsentrasi pada salah satu sisi dan kurvanya akan menceng.

Ukuran kemiringan kurva adalah derajat atau ukuran dari ketidaksimetrian suatu distribusi data. Nilai skewness (ukuran kemiringan) menunjukkan data normal ketika nilai-nilai tersebut berada di antara rentang nilai -2 sampai dengan 2. Kurva positif apabila rata-rata hitung > modus/median. Kurva negative apabila rata-rata hitung < modus/media

sumber : https://accounting.binus.ac.id/2021/08/12/memahami-nilai-skewness-ukuran-kemiringan-dalam-statistik-deskriptif/#:~:text=Skewness%20diartikan%20sebagai%20kemiringan%20distribusi,sisi%20dan%20kurvanya%20akan%20menceng.
"""

import seaborn as sns
import matplotlib.pyplot as plt
#cek skewness data
# Visualisasi Semua distribusi
numerical_cols = df.select_dtypes(include=np.number).columns
for i in numerical_cols:
  sns.histplot(df[i], kde=True)
  plt.title(f'Distribusi data {i}')
  plt.xlabel(i)
  plt.ylabel('Frekuensi')
  plt.show()

  # Cek skewness
  skewness = df[i].skew()
  print(f'Skewness: {skewness:.4f}')

  #print median, modus dan mean nya
  print("Median:", df[i].median())
  print("Modus:", df[i].mode()[0])
  print("Mean:", df[i].mean())

  # Interpretasi
  if skewness > 0:
      print('Distribusi data Price cenderung right-skewed (positive skew).')
  elif skewness < 0:
      print('Distribusi data Price cenderung left-skewed (negative skew).')
  else:
      print('Distribusi data Price cenderung simetris (mendekati Gaussian).')

# Pilih kolom numerik
numerical_cols = df.select_dtypes(include=np.number).columns

# Buat boxplot untuk setiap kolom numerik
for col in numerical_cols:
    plt.figure(figsize=(8, 6))  # Atur ukuran gambar
    sns.boxplot(y=df[col])  # Buat boxplot
    plt.title(f'Boxplot {col}')  # Judul boxplot
    plt.ylabel(col)  # Label sumbu y
    plt.show()  # Tampilkan boxplot

outlier_features = []
for col in numerical_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    if df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0] > 0:
        outlier_features.append(col)

print("Fitur yang memiliki outlier:", outlier_features)

import pandas as pd

# Assuming 'df' is your DataFrame
Q1 = df['Engine'].quantile(0.25)
Q3 = df['Engine'].quantile(0.75)
IQR = Q3 - Q1

# Batas bawah dan atas
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Mencari outlier
outliers = df[(df['Engine'] < lower_bound) | (df['Engine'] > upper_bound)]['Engine']

# Menampilkan nilai outlier
print("Nilai outlier di feature 'Engine':")
print(outliers)

import pandas as pd

# Assuming 'df' is your DataFrame
Q1 = df['Power'].quantile(0.25)
Q3 = df['Power'].quantile(0.75)
IQR = Q3 - Q1

# Batas bawah dan atas
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Mencari outlier
outliers = df[(df['Power'] < lower_bound) | (df['Power'] > upper_bound)]['Power']

# Menampilkan nilai outlier
print("Nilai outlier di feature 'Power':")
print(outliers)

df.head()

"""##Feature Engineering"""

#code goes here
df.head()

# Cek missing values
print("Missing values sebelum penanganan:")
print(df.isnull().sum())

# Mengisi missing values
df['Seats'].fillna(df['Seats'].mode()[0], inplace=True)
df['Engine'].fillna(df['Engine'].median(), inplace=True)
df['Power'].fillna(df['Power'].median(), inplace=True)

df.dropna(subset=['Price'], inplace=True)

df.isna().sum()

df['Price'].head()

# Pembuatan fitur
# Mobil Tua = Harga semakin turun
df['Car_Age'] = 2025 - df['Year']

# Rasio power per kapasitas mesin
df['Power_per_CC'] = df['Power'] / df['Engine']

df.head(5)

# One hot encoding untuk fitur kategorikal karena sebelum nya kategori nya terlalu banyak
df = pd.get_dummies(df, columns=['Fuel_Type', 'Transmission', 'Owner_Type'], drop_first=True)

from sklearn.preprocessing import LabelEncoder

# Brand dan Model punya terlalu banyak kategori jadi menggunakan label enoding untuk mengubah teks menjadi angka berdasarkan urutan kemunculannya
le_brand = LabelEncoder()
le_model = LabelEncoder()

df['Brand'] = le_brand.fit_transform(df['Brand'])
df['Model'] = le_model.fit_transform(df['Model'])

# # Scaling untuk Fitur numerik
# from sklearn.preprocessing import StandardScaler

# # StandardScaler menstandarisasi data ke distribusi normal (mean 0, std 1)
# numerical_cols = ['Kilometers_Driven', 'Mileage', 'Engine', 'Power', 'Car_Age', 'Power_per_CC']
# scaler = StandardScaler()
# df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print("Data akhir siap modeling:")
print(df.head())
print("Bentuk dataset:", df.shape)

import matplotlib.pyplot as plt
import seaborn as sns

# Hitung korelasi antar fitur
correlation_matrix = df.corr()

# Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Heatmap Korelasi antar Fitur", fontsize=16)
plt.show()

# Fitur yang paling berpengaruh terhadap harga
selected_features = [
    'Power',
    'Engine',
    'Car_Age',
    'Power_per_CC',
    'Mileage',
    'Transmission_1'
]

X = df[selected_features]
y = df['Price']

df.head()

"""#Model Buiding

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score

warnings.filterwarnings("ignore")

# ---------------------------
# 1. Preprocessing Function
# ---------------------------
def preprocess_data(df, target_col='Price', test_size=0.3):
    X = df.drop(columns=[target_col])
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, shuffle=True
    )

    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)

    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()
    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()

    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test

# ---------------------------
# 2. Train Random Forest Model
# ---------------------------
def train_rf_model(X_train, y_train, n_estimators=200 ):
    model = RandomForestRegressor(max_depth= 10, max_features= 0.5, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200)
    model.fit(X_train, y_train)
    return model

# ---------------------------
# 3. Cross Validation
# ---------------------------
def cross_validate_model(model, X, y, folds=5):
    kf = KFold(n_splits=folds, shuffle=True, random_state=42)
    scorer = make_scorer(mean_squared_error, greater_is_better=False)
    cv_scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)
    print("Cross-Validation MSE (per fold):", -cv_scores)
    print("Average CV MSE:", -np.mean(cv_scores))

# ---------------------------
# 4. Evaluation Function
# ---------------------------
def evaluate_model(y_true_scaled, y_pred_scaled, scaler_y):
    y_true = scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1))
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R² Score: {r2:.4f}")

    return y_true, y_pred

# ---------------------------
# 5. Plot Prediction
# ---------------------------
def plot_predictions(y_true, y_pred):
    plt.figure(figsize=(10,6))
    plt.plot(y_true, label='Data Aktual', color='blue')
    plt.plot(y_pred, label='Prediksi Random Forest', color='orange')
    plt.title('Prediksi Harga Mobil Menggunakan Random Forest')
    plt.xlabel('Index Data')
    plt.ylabel('Harga Mobil')
    plt.legend()
    plt.tight_layout()
    plt.show()

# ==============================
# MAIN PIPELINE
# ==============================

# Asumsikan df sudah tersedia
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test_raw = preprocess_data(df)

# Train Random Forest
rf_model = train_rf_model(X_train_scaled, y_train_scaled)

# Cross-Validation
cross_validate_model(rf_model, X_train_scaled, y_train_scaled)

# Prediksi untuk data train
y_train_pred_scaled = rf_model.predict(X_train_scaled)

# Prediction
y_pred_scaled = rf_model.predict(X_test_scaled)

# Evaluation
y_true, y_pred = evaluate_model(y_test_scaled, y_pred_scaled, scaler_y)

# R-squared untuk data train
r2_train = r2_score(y_train_scaled, y_train_pred_scaled)
print(f"R-squared (Train): {r2_train:.4f}")

# R-squared untuk data test
r2_test = r2_score(y_test_scaled, y_pred_scaled)
print(f"R-squared (Test): {r2_test:.4f}")

# Visualization
plot_predictions(y_true, y_pred)

from sklearn.model_selection import cross_val_score
cv_r2 = cross_val_score(
    rf_model, X, y, cv=5, # Replace 'model' with 'rf_model'
    scoring="r2", n_jobs=-1
)
print("R² CV per fold:", cv_r2)
print("Average R² CV  :", cv_r2.mean())

-S#Hyperparameter tuning
from sklearn.model_selection import GridSearchCV

def tune_rf_model(X_train, y_train, cv=5, n_jobs=-1):
    """
    Cari kombinasi parameter terbaik untuk RandomForestRegressor
    menggunakan GridSearchCV.
    """
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['auto', 'sqrt', 0.5]
    }

    base_rf = RandomForestRegressor(random_state=42)
    grid = GridSearchCV(
        estimator=base_rf,
        param_grid=param_grid,
        scoring='r2',
        cv=cv,
        n_jobs=n_jobs,
        verbose=1
    )
    grid.fit(X_train, y_train)

    print("Best params   :", grid.best_params_)
    print("Best R² (CV)  :", grid.best_score_)
    return grid.best_estimator_

# ======================================
#    GANTI BAGIAN TRAINING di pipeline
# ======================================
# X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test_raw = preprocess_data(df)

# Dari:
# rf_model = train_rf_model(X_train_scaled, y_train_scaled)

# Menjadi:
rf_model = tune_rf_model(X_train_scaled, y_train_scaled)

# Lalu lanjutkan sisa pipeline:
cross_validate_model(rf_model, X_train_scaled, y_train_scaled)
y_train_pred_scaled = rf_model.predict(X_train_scaled)
y_pred_scaled       = rf_model.predict(X_test_scaled)
y_true, y_pred      = evaluate_model(y_test_scaled, y_pred_scaled, scaler_y)

r2_train = r2_score(y_train_scaled, y_train_pred_scaled)
print(f"R-squared (Train): {r2_train:.4f}")
r2_test  = r2_score(y_test_scaled, y_pred_scaled)
print(f"R-squared (Test) : {r2_test:.4f}")

plot_predictions(y_true, y_pred)

cv_r2 = cross_val_score(
    rf_model, X_train_scaled, y_train_scaled,
    cv=5, scoring="r2", n_jobs=-1
)
print("R² CV per fold:", cv_r2)
print("Average R² CV  :", cv_r2.mean())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import time

from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score

warnings.filterwarnings("ignore")

# ---------------------------
# 1. Preprocessing
# ---------------------------
def preprocess_data(df, target_col='Price', test_size=0.3):
    X = df.drop(columns=[target_col])
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, shuffle=True
    )

    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)

    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()
    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()

    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test

# Catat waktu mulai
start_time = time.time()

# ---------------------------
# 2. Train XGBoost Model
# ---------------------------
def train_xgb_model(X_train, y_train, params=None):
    if params is None:
        params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 4,
            'random_state': 42
        }
    model = XGBRegressor(**params)
    model.fit(X_train, y_train)
    return model

# ---------------------------
# 3. Cross Validation
# ---------------------------
def cross_validate_model(model, X, y, folds=5):
    kf = KFold(n_splits=folds, shuffle=True, random_state=42)
    scorer = make_scorer(mean_squared_error, greater_is_better=False)
    cv_scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)
    print("Cross-Validation MSE (per fold):", -cv_scores)
    print("Average CV MSE:", -np.mean(cv_scores))

# ---------------------------
# 4. Evaluation
# ---------------------------
def evaluate_model(y_true_scaled, y_pred_scaled, scaler_y):
    y_true = scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1))
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R² Score: {r2:.4f}")

    return y_true, y_pred

# ---------------------------
# 5. Plotting
# ---------------------------
def plot_predictions(y_true, y_pred):
    plt.figure(figsize=(10,6))
    plt.plot(y_true, label='Data Aktual', color='blue')
    plt.plot(y_pred, label='Prediksi XGBoost', color='green')
    plt.title('Prediksi Harga Mobil Menggunakan XGBoost')
    plt.xlabel('Index Data')
    plt.ylabel('Harga Mobil')
    plt.legend()
    plt.tight_layout()
    plt.show()

# Catat waktu selesai
end_time = time.time()

# Hitung waktu pelatihan
training_time = end_time - start_time

# Tampilkan waktu pelatihan
print(f"Waktu pelatihan: {training_time:.4f} detik")

# ==============================
# MAIN PIPELINE
# ==============================

# Asumsikan df sudah tersedia
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test_raw = preprocess_data(df)

# Train XGBoost Model
xgb_model = train_xgb_model(X_train_scaled, y_train_scaled)

# Cross-Validation
cross_validate_model(xgb_model, X_train_scaled, y_train_scaled)

# Prediction
y_pred_scaled = xgb_model.predict(X_test_scaled)

# Prediksi untuk data train
y_train_pred_scaled = xgb_model.predict(X_train_scaled)

# R-squared untuk data train
r2_train = r2_score(y_train_scaled, y_train_pred_scaled)
print(f"R-squared (Train): {r2_train:.4f}")

# R-squared untuk data test
r2_test = r2_score(y_test_scaled, y_pred_scaled)
print(f"R-squared (Test): {r2_test:.4f}")

# Evaluation
y_true, y_pred = evaluate_model(y_test_scaled, y_pred_scaled, scaler_y)

# Visualization
plot_predictions(y_true, y_pred)

import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore")

# ---------------------------
# 1. Preprocessing Function
# ---------------------------
def preprocess_data(df, target_col='Price', test_size=0.3):
    X = df.drop(columns=[target_col])
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, shuffle=True
    )

    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)

    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()
    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()

    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test

# ---------------------------
# 2. Model Training Function
# ---------------------------
def train_svr_model(X_train, y_train, C=1000, gamma=0.01, epsilon=0.1):
    svr = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)
    svr.fit(X_train, y_train)
    return svr

# ---------------------------
# 3. Cross Validation (K-Fold)
# ---------------------------
def cross_validate_model(model, X, y, folds=5):
    kf = KFold(n_splits=folds, shuffle=True, random_state=42)
    scorer = make_scorer(mean_squared_error, greater_is_better=False)
    cv_scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)
    print("Cross-Validation MSE (per fold):", -cv_scores)
    print("Average CV MSE:", -np.mean(cv_scores))

# ---------------------------
# 4. Evaluation Function
# ---------------------------
def evaluate_model(y_true_scaled, y_pred_scaled, scaler_y):
    y_true = scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1))
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R² Score: {r2:.4f}")

    return y_true, y_pred

# ---------------------------
# 5. Visualization
# ---------------------------
def plot_predictions(y_true, y_pred):
    plt.figure(figsize=(10,6))
    plt.plot(y_true, label='Data Aktual', color='blue')
    plt.plot(y_pred, label='Prediksi SVR', color='red')
    plt.title('Prediksi Harga Mobil Menggunakan SVR')
    plt.xlabel('Index Data')
    plt.ylabel('Harga Mobil')
    plt.legend()
    plt.tight_layout()
    plt.show()

# ==============================
# MAIN PIPELINE
# ==============================

# Asumsikan df sudah tersedia
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_y, y_test_raw = preprocess_data(df)

# Model Training
svr_model = train_svr_model(X_train_scaled, y_train_scaled)

# Cross-Validation (K-Fold)
cross_validate_model(svr_model, X_train_scaled, y_train_scaled)

# Prediksi untuk data train
y_train_pred_scaled = svr_model.predict(X_train_scaled)

# Prediction
y_pred_scaled = svr_model.predict(X_test_scaled)

# Evaluation
y_true, y_pred = evaluate_model(y_test_scaled, y_pred_scaled, scaler_y)

# R-squared untuk data train
r2_train = r2_score(y_train_scaled, y_train_pred_scaled)
print(f"R-squared (Train): {r2_train:.4f}")

# R-squared untuk data test
r2_test = r2_score(y_test_scaled, y_pred_scaled)
print(f"R-squared (Test): {r2_test:.4f}")

# Plot
plot_predictions(y_true, y_pred)

"""Model | Akurasi | Interpretasi | Kecepatan | Kelebihan Utama

Linear Regression | ❗️Cukup | ✅ Mudah | ✅ Cepat | Sangat mudah dipahami

Random Forest | ✅ Bagus | ⚠️ Sedang | ⚠️ Sedang | Robust, tangkap non-linearitas

SVR | ⚠️ Bagus | ⚠️ Sulit | ❌ Lambat | Presisi tinggi, cocok dataset kecil

XGBoost | ✅⚡ Sangat Bagus | ❌ Sulit | ⚠️ Sedang | Performa kompetisi, sangat powerful
"""